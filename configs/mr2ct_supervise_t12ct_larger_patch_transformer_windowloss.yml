desc: "mr2ct supervised training using one encoder-decoder network"

random_seed: 42
spatial_dim: 3
in_channel: 1   # in_channel is the same as the out_channel in cyclegan


# =======================
# === model architecture ==
# =======================
# generators
model_name: shuffleunet  # shuffleunet, unet
transformer_layers: 2
num_residual_units: 0
shuffleunet_filters: [64, 128, 256, 384, 384]
shuffleunet_strides: [2, 2, 2, 2]
filters: [64, 96, 128, 192, 256, 384, 512, 768, 1024]  # filters reference from Optimized U-Net for Brain Tumor Segmentation <https://arxiv.org/pdf/2110.03352.pdf>
kernel_size: 
  - [3, 3, 3]
  - [3, 3, 3]
  - [3, 3, 3]
  - [3, 3, 3]
  - [3, 3, 3]
  - [3, 3, 3]
strides:
  - [1, 1, 1]
  - [2, 2, 1]
  - [2, 2, 1]
  - [2, 2, 2]
  - [2, 2, 2]
  - [2, 2, 2]
unet_use_resblock: True

# =======================
# === Optimizers ========
# =======================

# optimizer
generator:
  name: adamw # sgd, adamw, rangerlars
  base_lr: 0.003    # will be scaled by the number of GPUs, currently ban
  betas: [0.9, 0.999]
  weight_decay: 0.
  # lr_scheduler:
  warmup_epochs: 1

# =======================
# === Datasets ========
# =======================
# the data architecture should be like this:
# data_root_dir
# ├── pipeline_niigz_betneck
# │   ├── mr
# │   │   ├── 0001.nii.gz
# │   │   ├── 0002.nii.gz
# │   │   └── ...
# │   └── ct
# │       ├── 0001.nii.gz
# │       ├── 0002.nii.gz
# │       └── ...
# └── resampled_pipeline_niigz_betneck_brainsegs
#     └── mr
#         ├── 0001_brainseg.nii.gz
#         ├── 0002_brainseg.nii.gz
#         └── ...
file_suffix: ".nii.gz"
data_root_dir: data
image_subfolder_name: ct_reg2_mr_betneck          # contains the mr and ct subfolders
data_json_path: data/cross_validation_t12ct/cross_validation_fold_0.json    # if no json found in this path, it will be created in training
pix_dim: [-1, -1, 1.0]
img_centercrop_size: [576, 576, 192]  # 480, 576, 448
use_patch: True
patch_size: [576, 576, 64]   # [384, 416, 56]
patch_n: 1
batch_size: 1
workers: 8


# =======================
# === Training ========
# =======================
total_epochs: 2000
save_checkpoint_freq: 10
criterion: l1
lambda_l1: 20.0
lambda_ssim_3d: 5.0
lambda_ssim_yz: 5.0
lambda_ssim_xz: 5.0
lambda_ssim_xy: 5.0

scale_window_0_100: 0.5
scale_window_100_1500: 0.5